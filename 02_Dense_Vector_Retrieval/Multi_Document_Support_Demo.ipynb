{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Document Support Demo\n",
    "\n",
    "This notebook demonstrates how to use the enhanced multi-document support in the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from aimakerspace.text_utils import PDFFileLoader, CharacterTextSplitter, RecursiveTextSplitter\nfrom aimakerspace.vectordatabase import VectorDatabase\nimport asyncio\nimport nest_asyncio\nnest_asyncio.apply()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Multiple Documents with Metadata\n",
    "\n",
    "The `PDFFileLoader` can now load multiple documents and track metadata for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load a single document\n",
    "pdf_loader = PDFFileLoader(\"data/Stone Ridge 2025 Investor Letter.pdf\")\n",
    "docs_with_metadata = pdf_loader.load_documents_with_metadata()\n",
    "\n",
    "print(f\"Loaded {len(docs_with_metadata)} document(s)\")\n",
    "for doc, meta in docs_with_metadata:\n",
    "    print(f\"  - {meta['source']}: {len(doc)} chars, {meta['num_pages']} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Load multiple documents from a directory\n",
    "# pdf_loader = PDFFileLoader(\"data/investor_letters/\")  # Will load all PDFs\n",
    "# docs_with_metadata = pdf_loader.load_documents_with_metadata()\n",
    "\n",
    "# print(f\"Loaded {len(docs_with_metadata)} document(s)\")\n",
    "# for doc, meta in docs_with_metadata:\n",
    "#     print(f\"  - {meta['source']}: {len(doc)} chars, {meta['num_pages']} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Split Documents While Preserving Metadata\n",
    "\n",
    "The `CharacterTextSplitter` now has a `split_texts_with_metadata()` method that preserves document source information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract texts and metadata separately\ntexts = [doc for doc, _ in docs_with_metadata]\nmetadatas = [meta for _, meta in docs_with_metadata]\n\n# Split with metadata - using CharacterTextSplitter (default)\ntext_splitter = CharacterTextSplitter()\nchunks_with_metadata = text_splitter.split_texts_with_metadata(texts, metadatas)\n\n# OR use RecursiveTextSplitter for better semantic chunks:\n# text_splitter = RecursiveTextSplitter()\n# chunks_with_metadata = text_splitter.split_texts_with_metadata(texts, metadatas)\n\nprint(f\"Created {len(chunks_with_metadata)} chunks\")\nprint(f\"\\nFirst chunk metadata: {chunks_with_metadata[0]['metadata']}\")\nprint(f\"First chunk text preview: {chunks_with_metadata[0]['text'][:100]}...\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 2.5: Chunking Strategy Comparison\n\nLet's compare two different chunking strategies to see how they affect chunk quality.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### How RecursiveTextSplitter Works\n\nThe `RecursiveTextSplitter` uses a hierarchy of separators:\n\n1. **First** tries to split on `\\n\\n` (paragraphs)\n2. **Then** tries `\\n` (lines) if chunks too large\n3. **Then** tries `. ` (sentences)\n4. **Then** tries ` ` (words)\n5. **Finally** splits characters as last resort\n\nThis preserves semantic meaning and produces better chunks for retrieval!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "### Strategy 1: CharacterTextSplitter (Simple)\n# Blindly splits every N characters - may break words/sentences\nchar_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\nchar_chunks = char_splitter.split_texts_with_metadata(texts, metadatas)\n\nprint(\"CharacterTextSplitter Results:\")\nprint(f\"  Total chunks: {len(char_chunks)}\")\nprint(f\"  Avg size: {sum(len(c['text']) for c in char_chunks) / len(char_chunks):.0f} chars\")\nprint(f\"\\n  Example chunk ending:\")\nprint(f\"  ...{char_chunks[5]['text'][-100:]}\")\n\n### Strategy 2: RecursiveTextSplitter (Sophisticated)\n# Respects paragraphs, sentences, and words\nrecursive_splitter = RecursiveTextSplitter(chunk_size=500, chunk_overlap=50)\nrecursive_chunks = recursive_splitter.split_texts_with_metadata(texts, metadatas)\n\nprint(f\"\\n{'='*80}\")\nprint(\"RecursiveTextSplitter Results:\")\nprint(f\"  Total chunks: {len(recursive_chunks)}\")\nprint(f\"  Avg size: {sum(len(c['text']) for c in recursive_chunks) / len(recursive_chunks):.0f} chars\")\nprint(f\"\\n  Example chunk ending:\")\nprint(f\"  ...{recursive_chunks[5]['text'][-100:]}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"Key Difference:\")\nprint(\"  CharacterTextSplitter: Often cuts mid-word/sentence\")\nprint(\"  RecursiveTextSplitter: Ends at natural boundaries (better for semantic search)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Vector Database with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vector database with metadata\n",
    "vector_db = VectorDatabase()\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list_with_metadata(chunks_with_metadata))\n",
    "\n",
    "print(f\"Vector database built with {len(vector_db.vectors)} chunks\")\n",
    "print(f\"Metadata stored for {len(vector_db.metadata)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Search with Metadata\n",
    "\n",
    "Now when we search, we can retrieve the document source information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with metadata\n",
    "results = vector_db.search_by_text(\n",
    "    \"What is Stone Ridge's investment philosophy?\",\n",
    "    k=3,\n",
    "    return_metadata=True\n",
    ")\n",
    "\n",
    "print(\"Search Results:\\n\")\n",
    "for i, (text, score, metadata) in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"  Source: {metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"  Chunk: {metadata.get('chunk_index', '?')}/{metadata.get('total_chunks', '?')}\")\n",
    "    print(f\"  Score: {score:.4f}\")\n",
    "    print(f\"  Text preview: {text[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Enhanced RAG Pipeline with Document Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimakerspace.openai_utils.prompts import UserRolePrompt, SystemRolePrompt\n",
    "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
    "\n",
    "class MultiDocumentRAGPipeline:\n",
    "    \"\"\"Enhanced RAG pipeline that tracks document sources.\"\"\"\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI, vector_db_retriever: VectorDatabase, \n",
    "                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "        self.response_style = response_style\n",
    "        self.include_scores = include_scores\n",
    "\n",
    "    def run_pipeline(self, user_query: str, k: int = 4, **system_kwargs) -> dict:\n",
    "        # Retrieve relevant contexts WITH metadata\n",
    "        context_list = self.vector_db_retriever.search_by_text(\n",
    "            user_query, k=k, return_metadata=True\n",
    "        )\n",
    "        \n",
    "        context_prompt = \"\"\n",
    "        similarity_scores = []\n",
    "        sources = []\n",
    "        \n",
    "        for i, (context, score, metadata) in enumerate(context_list, 1):\n",
    "            source = metadata.get('source', 'Unknown')\n",
    "            chunk_info = f\"{metadata.get('chunk_index', '?')+1}/{metadata.get('total_chunks', '?')}\"\n",
    "            \n",
    "            context_prompt += f\"[Source {i} - {source} (chunk {chunk_info})]: {context}\\n\\n\"\n",
    "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
    "            sources.append({\n",
    "                \"source\": source,\n",
    "                \"chunk_index\": metadata.get('chunk_index'),\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "        \n",
    "        # Create prompts\n",
    "        system_template = \"\"\"You are a helpful assistant that answers questions based on provided context.\n",
    "        Always cite which source document your information comes from.\n",
    "        Keep responses {response_style}.\"\"\"\n",
    "        \n",
    "        user_template = \"\"\"Context Information:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Please provide your answer and cite which source documents you used.\"\"\"\n",
    "        \n",
    "        system_prompt = SystemRolePrompt(system_template)\n",
    "        user_prompt = UserRolePrompt(user_template)\n",
    "        \n",
    "        formatted_system = system_prompt.create_message(response_style=self.response_style)\n",
    "        formatted_user = user_prompt.create_message(\n",
    "            context=context_prompt.strip(),\n",
    "            user_query=user_query\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"response\": self.llm.run([formatted_system, formatted_user]),\n",
    "            \"context\": context_list,\n",
    "            \"sources\": sources,\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": similarity_scores if self.include_scores else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the enhanced pipeline\n",
    "chat_openai = ChatOpenAI()\n",
    "rag_pipeline = MultiDocumentRAGPipeline(\n",
    "    vector_db_retriever=vector_db,\n",
    "    llm=chat_openai,\n",
    "    response_style=\"detailed\",\n",
    "    include_scores=True\n",
    ")\n",
    "\n",
    "result = rag_pipeline.run_pipeline(\n",
    "    \"What are the key investment themes discussed in the letter?\",\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(result['response'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nSources Used:\")\n",
    "for source in result['sources']:\n",
    "    print(f\"  - {source['source']} (chunk {source['chunk_index']}, score: {source['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Enhanced Features Summary\n\n### 1. Multi-Document Support\n- **Document Attribution**: Know which document each answer came from\n- **Historical Comparison**: Compare answers across multiple years of investor letters\n- **Source Filtering**: Can add filtering by document name or metadata\n- **Transparency**: Users can verify sources and context\n- **Better Context**: Understand temporal context (e.g., \"In the 2024 letter...\")\n\n### 2. Advanced Chunking Strategies\n- **CharacterTextSplitter**: Simple, fast, predictable chunk sizes\n- **RecursiveTextSplitter**: Respects natural language boundaries (paragraphs, sentences, words)\n- Choose the right strategy based on your use case:\n  - Use RecursiveTextSplitter for better semantic search quality\n  - Use CharacterTextSplitter for speed and simplicity"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}