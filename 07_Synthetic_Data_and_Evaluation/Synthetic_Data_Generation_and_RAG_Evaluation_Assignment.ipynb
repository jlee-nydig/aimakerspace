{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7: Synthetic Data Generation and RAG Evaluation with Ragas\n",
    "\n",
    "In this notebook, we'll go end-to-end from **generating synthetic evaluation data** to **systematically evaluating and improving a RAG pipeline** ‚Äî all using [Ragas](https://github.com/explodinggradients/ragas).\n",
    "\n",
    "The flow is:\n",
    "1. **Generate** synthetic test data using Ragas' knowledge graph-based approach\n",
    "2. **Build** a baseline RAG application with LangChain and LangGraph\n",
    "3. **Evaluate** the RAG application against our synthetic test set using Ragas metrics\n",
    "4. **Iterate** on the pipeline and measure the impact\n",
    "\n",
    "> **NOTE:** Ragas is framework-agnostic ‚Äî while this example uses LangChain/LangGraph, you can use Ragas with any framework (or none at all). Ragas is best suited for finding *directional* changes in your LLM-based systems. The absolute scores aren't comparable in a vacuum.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Part 1: Synthetic Data Generation**\n",
    "- Task 1: Dependencies and API Keys\n",
    "- Task 2: Data Preparation\n",
    "- Task 3: Knowledge Graph Construction\n",
    "- Task 4: Generating Synthetic Test Data\n",
    "- ***‚ùì Question #1 & Question #2***\n",
    "- ***üèóÔ∏è Activity #1: Custom Query Distribution***\n",
    "\n",
    "**Part 2: RAG Evaluation with Ragas**\n",
    "- Task 5: Building a Baseline RAG Application\n",
    "- Task 6: Evaluating with Ragas\n",
    "- Task 7: Making Adjustments and Re-Evaluating\n",
    "- ***‚ùì Question #3, Question #4, Question #5, & Question #6***\n",
    "- ***üèóÔ∏è Activity #2: Implement a Different Reranking Strategy***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Synthetic Data Generation with Ragas\n",
    "\n",
    "Before we can evaluate a RAG system, we need high-quality test data. Manually creating question-answer pairs is time-consuming and often biased toward simple queries. Ragas solves this by building a **knowledge graph** from your documents and using it to generate diverse, realistic test questions automatically.\n",
    "\n",
    "We'll use the **Stone Ridge 2025 Investor Letter** and an **Alternative Investments Handbook** as our source documents ‚Äî maintaining continuity with the investment advisory use case from previous sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Dependencies and API Keys\n",
    "\n",
    "If you have not already done so, install the required libraries using the uv package manager:\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "We'll need API keys for:\n",
    "- **OpenAI** ‚Äî for LLM and embedding models (used in both SDG and RAG evaluation)\n",
    "- **Cohere** ‚Äî for reranking in the improved pipeline ([sign up here](https://docs.cohere.com/reference/about))\n",
    "\n",
    "You have two options for supplying your API keys:\n",
    "- Use environment variables (copy `.env.sample` to `.env` and fill in your keys)\n",
    "- Provide them via the prompts below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/chris/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Please enter your OpenAI API key!\")\n",
    "\n",
    "if not os.environ.get(\"COHERE_API_KEY\"):\n",
    "    os.environ[\"COHERE_API_KEY\"] = getpass(\"Please enter your Cohere API key!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Data Preparation\n",
    "\n",
    "We'll prepare our data using two complementary investment-focused sources:\n",
    "- **Stone Ridge 2025 Investor Letter** ‚Äî covering Stone Ridge's investment philosophy, Bayesian approach to decision-making, energy investments, reinsurance, and risk management\n",
    "- **Alternative Investments Handbook** ‚Äî covering alternative asset classes including real estate, private equity, hedge funds, reinsurance, commodities, and infrastructure\n",
    "\n",
    "The topical overlap between these documents (particularly around reinsurance, risk premiums, diversification, and alternative investments) helps Ragas build rich cross-document relationships in the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 documents:\n",
      "  - Stone Ridge 2025 Investor Letter: 14 pages\n",
      "  - AlternativeInvestmentsHandbook.txt: 1 document(s)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader\n",
    "\n",
    "# Load the Stone Ridge 2025 Investor Letter (PDF)\n",
    "pdf_loader = PyMuPDFLoader(\"data/Stone Ridge 2025 Investor Letter.pdf\")\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "# Load the Alternative Investments Handbook (text)\n",
    "txt_loader = TextLoader(\"data/AlternativeInvestmentsHandbook.txt\")\n",
    "txt_docs = txt_loader.load()\n",
    "\n",
    "# Combine into a single list\n",
    "docs = pdf_docs + txt_docs\n",
    "print(f\"Loaded {len(docs)} documents:\")\n",
    "print(f\"  - Stone Ridge 2025 Investor Letter: {len(pdf_docs)} pages\")\n",
    "print(f\"  - AlternativeInvestmentsHandbook.txt: {len(txt_docs)} document(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Knowledge Graph Construction\n",
    "\n",
    "Ragas uses a **knowledge graph-based approach** to create synthetic test data. This is powerful because it allows us to create complex, multi-hop queries ‚Äî not just simple factoid questions. Systems tend to perform well on simple evaluation tasks, so this additional complexity helps us find real weaknesses.\n",
    "\n",
    "The process works in three stages:\n",
    "1. **Build the graph** ‚Äî insert documents as nodes\n",
    "2. **Apply transformations** ‚Äî extract headlines, summaries, themes, entities, and embeddings\n",
    "3. **Create relationships** ‚Äî use cosine similarity and overlap scores to connect related nodes\n",
    "\n",
    "Let's start by defining our `generator_llm` and `generator_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialize the Knowledge Graph\n",
    "\n",
    "We create an empty knowledge graph and populate it with our document nodes. Each full document becomes a node of type `DOCUMENT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 15, relationships: 0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph, Node, NodeType\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "for doc in docs:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "        )\n",
    "    )\n",
    "kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Apply Transformations\n",
    "\n",
    "Now we apply the [default transformations](https://docs.ragas.io/en/latest/references/transforms/#ragas.testset.transforms.default_transforms) to enrich our knowledge graph. These transformations:\n",
    "\n",
    "- **HeadlinesExtractor** ‚Äî finds the overall headlines for each document\n",
    "- **SummaryExtractor** ‚Äî produces summaries of the documents\n",
    "- **ThemesExtractor** ‚Äî extracts broad themes\n",
    "- **EmbeddingExtractor** ‚Äî creates embeddings for similarity computation\n",
    "- **NERExtractor** ‚Äî extracts named entities\n",
    "\n",
    "These are then used to build relationships between nodes via cosine similarity and overlap scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93472abf59934b11965c85e0fe8ef34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fdbb12a08e4c8e9aeafbbb5df48811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: 'headlines' property not found in this node\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75a095fa9dc4496a4ddcfbfb5c964f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary' already exists in node '2db385'. Skipping!\n",
      "Property 'summary' already exists in node '5bb9b5'. Skipping!\n",
      "Property 'summary' already exists in node 'b61634'. Skipping!\n",
      "Property 'summary' already exists in node 'e46f0d'. Skipping!\n",
      "Property 'summary' already exists in node '078d76'. Skipping!\n",
      "Property 'summary' already exists in node 'd33ea7'. Skipping!\n",
      "Property 'summary' already exists in node 'ca0164'. Skipping!\n",
      "Property 'summary' already exists in node '6e2f7d'. Skipping!\n",
      "Property 'summary' already exists in node '0c8363'. Skipping!\n",
      "Property 'summary' already exists in node 'cb09ce'. Skipping!\n",
      "Property 'summary' already exists in node 'e4dabf'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9075848b7043a9add69486a0513ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34dec85f82249c5851e035175c8a45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary_embedding' already exists in node '5bb9b5'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '2db385'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '6e2f7d'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'e46f0d'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'b61634'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'ca0164'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '0c8363'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '078d76'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'd33ea7'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'cb09ce'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'e4dabf'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ca217791c146ccb919016516094cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 35, relationships: 303)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "transforms = default_transforms(\n",
    "    documents=docs,\n",
    "    llm=generator_llm,\n",
    "    embedding_model=generator_embeddings\n",
    ")\n",
    "apply_transforms(kg, transforms)\n",
    "kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Save the Knowledge Graph\n",
    "\n",
    "Knowledge graphs can be saved and loaded, which is useful for iterating on test generation without rebuilding the graph each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg.save(\"investment_data_kg.json\")\n",
    "\n",
    "# You can reload it later:\n",
    "# kg = KnowledgeGraph.load(\"investment_data_kg.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Generating Synthetic Test Data\n",
    "\n",
    "With our knowledge graph built, we can now generate synthetic test data. Ragas provides several **query synthesizers**, each producing a different type of question:\n",
    "\n",
    "- **`SingleHopSpecificQuerySynthesizer`** ‚Äî creates questions answerable from a single chunk of context (e.g., *\"What is Stone Ridge's approach to reinsurance investing?\"*)\n",
    "- **`MultiHopAbstractQuerySynthesizer`** ‚Äî creates questions requiring synthesis across multiple chunks at an abstract level (e.g., *\"How do alternative risk premiums relate to portfolio diversification?\"*)\n",
    "- **`MultiHopSpecificQuerySynthesizer`** ‚Äî creates questions requiring specific details from multiple chunks (e.g., *\"How does Stone Ridge's Bayesian philosophy connect to their energy investment strategy?\"*)\n",
    "\n",
    "We define a **query distribution** to control the mix of question types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    "    MultiHopAbstractQuerySynthesizer,\n",
    "    MultiHopSpecificQuerySynthesizer,\n",
    ")\n",
    "\n",
    "query_distribution = [\n",
    "    (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5),\n",
    "    (MultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25),\n",
    "    (MultiHopSpecificQuerySynthesizer(llm=generator_llm), 0.25),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91a7eb993804210a7580523455f5eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0449a3228b5f438f907d26ff4c5e35a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea6f74f843a455ea5eaca53f7b5ef25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How has the city of Chicago influenced your ca...</td>\n",
       "      <td>[and my career has essentially been a three-de...</td>\n",
       "      <td>My career has essentially been a three-decade ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How did Fisher exemplify Bayesian principles i...</td>\n",
       "      <td>[TODAY‚ÄôS THE DAY Fancy math aside, the foundat...</td>\n",
       "      <td>Fisher exemplified Bayesian principles by usin...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the recent performance of Stone Ridge ...</td>\n",
       "      <td>[Standardized returns as of most recent quarte...</td>\n",
       "      <td>Stone Ridge Energy Equity Tranches had a stand...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What information does Chapter 1 of The Alterna...</td>\n",
       "      <td>[The Alternative Investments Handbook A Practi...</td>\n",
       "      <td>Chapter 1 explains that alternative investment...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What NOI means in real estate?</td>\n",
       "      <td>[PART 2: REAL ESTATE INVESTMENTS Chapter 4: Re...</td>\n",
       "      <td>NOI stands for Net Operating Income, which is ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How do commodities and real assets serve as an...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nPART 6: COMMODITIES AND REAL ASSET...</td>\n",
       "      <td>Commodities and real assets serve as an inflat...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the central bank demand for gold rela...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nPART 6: COMMODITIES AND REAL ASSET...</td>\n",
       "      <td>The central bank demand for gold, as part of t...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How learning from experience and updating beli...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nand my career has essentially been...</td>\n",
       "      <td>The context explains that Bayesian treasure hu...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How do Chapter 1's overview of alternative ass...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nPART 2: REAL ESTATE INVESTMENTS Ch...</td>\n",
       "      <td>Chapter 1 introduces alternative investments s...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In context of Chapter 4 and Chapter 13, how do...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nPART 6: COMMODITIES AND REAL ASSET...</td>\n",
       "      <td>The context from Chapter 4 discusses real esta...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wha is the importent of Private Equite in dive...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe Alternative Investments Handbo...</td>\n",
       "      <td>Private equity is a key component of diversifi...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   How has the city of Chicago influenced your ca...   \n",
       "1   How did Fisher exemplify Bayesian principles i...   \n",
       "2   What is the recent performance of Stone Ridge ...   \n",
       "3   What information does Chapter 1 of The Alterna...   \n",
       "4                      What NOI means in real estate?   \n",
       "5   How do commodities and real assets serve as an...   \n",
       "6   How does the central bank demand for gold rela...   \n",
       "7   How learning from experience and updating beli...   \n",
       "8   How do Chapter 1's overview of alternative ass...   \n",
       "9   In context of Chapter 4 and Chapter 13, how do...   \n",
       "10  Wha is the importent of Private Equite in dive...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [and my career has essentially been a three-de...   \n",
       "1   [TODAY‚ÄôS THE DAY Fancy math aside, the foundat...   \n",
       "2   [Standardized returns as of most recent quarte...   \n",
       "3   [The Alternative Investments Handbook A Practi...   \n",
       "4   [PART 2: REAL ESTATE INVESTMENTS Chapter 4: Re...   \n",
       "5   [<1-hop>\\n\\nPART 6: COMMODITIES AND REAL ASSET...   \n",
       "6   [<1-hop>\\n\\nPART 6: COMMODITIES AND REAL ASSET...   \n",
       "7   [<1-hop>\\n\\nand my career has essentially been...   \n",
       "8   [<1-hop>\\n\\nPART 2: REAL ESTATE INVESTMENTS Ch...   \n",
       "9   [<1-hop>\\n\\nPART 6: COMMODITIES AND REAL ASSET...   \n",
       "10  [<1-hop>\\n\\nThe Alternative Investments Handbo...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   My career has essentially been a three-decade ...   \n",
       "1   Fisher exemplified Bayesian principles by usin...   \n",
       "2   Stone Ridge Energy Equity Tranches had a stand...   \n",
       "3   Chapter 1 explains that alternative investment...   \n",
       "4   NOI stands for Net Operating Income, which is ...   \n",
       "5   Commodities and real assets serve as an inflat...   \n",
       "6   The central bank demand for gold, as part of t...   \n",
       "7   The context explains that Bayesian treasure hu...   \n",
       "8   Chapter 1 introduces alternative investments s...   \n",
       "9   The context from Chapter 4 discusses real esta...   \n",
       "10  Private equity is a key component of diversifi...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   single_hop_specifc_query_synthesizer  \n",
       "5   multi_hop_abstract_query_synthesizer  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_specific_query_synthesizer  \n",
       "9   multi_hop_specific_query_synthesizer  \n",
       "10  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(\n",
    "    llm=generator_llm,\n",
    "    embedding_model=generator_embeddings,\n",
    "    knowledge_graph=kg\n",
    ")\n",
    "\n",
    "testset = generator.generate(testset_size=10, query_distribution=query_distribution)\n",
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstracted SDG (Shortcut)\n",
    "\n",
    "The above was the **unrolled** process showing each step. Ragas also provides a one-liner that builds the knowledge graph under the hood and generates the test set in a single call. This is convenient for quick iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstracted approach (for reference):\n",
    "# generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "# dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Question #1:\n",
    "\n",
    "What are the three types of query synthesizers doing? Describe each one in simple terms.\n",
    "\n",
    "##### ‚úÖ Answer:\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Question #2:\n",
    "\n",
    "Ragas offers both an \"unrolled\" (manual) approach and an \"abstracted\" (automatic) approach to synthetic data generation. What are the trade-offs between these two approaches? When would you choose one over the other?\n",
    "\n",
    "##### ‚úÖ Answer:\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Activity #1: Custom Query Distribution\n",
    "\n",
    "Modify the `query_distribution` to experiment with different ratios of query types.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create a custom query distribution with different weights than the default\n",
    "2. Generate a new test set using your custom distribution\n",
    "3. Compare the types of questions generated with the default distribution\n",
    "4. Explain why you chose the weights you did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Define a custom query distribution with different weights\n",
    "# Generate a new test set and compare with the default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: RAG Evaluation with Ragas\n",
    "\n",
    "Now that we have our synthetic test data, we can use it to **systematically evaluate** a RAG pipeline. The idea is simple:\n",
    "1. Build a RAG application\n",
    "2. Run our synthetic queries through it\n",
    "3. Score the results using Ragas metrics\n",
    "4. Make changes and measure the impact\n",
    "\n",
    "This gives us a **data-driven approach** to improving our RAG system, rather than relying on vibes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Building a Baseline RAG Application\n",
    "\n",
    "We'll build a deliberately simple (and somewhat bad) RAG pipeline as our **baseline**, so we can clearly see the impact of improvements later.\n",
    "\n",
    "Our baseline uses:\n",
    "- Tiny chunks (50 characters) with no overlap\n",
    "- A small embedding model (`text-embedding-3-small`)\n",
    "- Only 3 retrieved documents\n",
    "- A basic prompt\n",
    "\n",
    "> **NOTE:** We use the same data that our synthetic test set was generated from ‚Äî this is required because the test questions are specifically designed for this investment data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R ‚Äî Retrieval\n",
    "\n",
    "First, we chunk our documents and build a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2045"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=0)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Question #3:\n",
    "\n",
    "What is the purpose of the `chunk_overlap` parameter in the `RecursiveCharacterTextSplitter`?\n",
    "\n",
    "##### ‚úÖ Answer:\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"baseline_rag\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"baseline_rag\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "_ = vector_store.add_documents(documents=split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A ‚Äî Augmented\n",
    "\n",
    "A simple RAG prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "You are a helpful investment advisory assistant who answers questions based on provided context. You must only use the provided context, and cannot use your own knowledge.\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G ‚Äî Generation\n",
    "\n",
    "We use `gpt-4.1-nano` for generation to avoid using the same model as our judge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"response\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the RAG Graph with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Stone Ridge's investment philosophy is centered on a relentless focus on growing.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is Stone Ridge's investment philosophy?\"})\n",
    "response[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tiny 50-character chunks and only 3 retrieved documents, the baseline likely struggles to provide good answers about Stone Ridge's investment philosophy. That's intentional ‚Äî it gives us room to improve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Evaluating with Ragas\n",
    "\n",
    "Now we can evaluate our baseline RAG against the synthetic test data we generated in Part 1.\n",
    "\n",
    "First, we run all the synthetic queries through our RAG pipeline to collect responses and retrieved contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_row in testset:\n",
    "    response = graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
    "    test_row.eval_sample.response = response[\"response\"]\n",
    "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to an `EvaluationDataset` for smoother evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(testset.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a **judge model** ‚Äî a separate, capable model that scores the outputs. Using a different model than the generator avoids self-evaluation bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Baseline Evaluation\n",
    "\n",
    "We evaluate across six metrics:\n",
    "- **Context Recall** ‚Äî did we retrieve the relevant context?\n",
    "- **Faithfulness** ‚Äî is the answer grounded in the retrieved context?\n",
    "- **Factual Correctness** ‚Äî is the answer factually correct vs. the reference?\n",
    "- **Answer Relevancy** ‚Äî is the answer relevant to the question?\n",
    "- **Context Entity Recall** ‚Äî did we capture the key entities from the reference context?\n",
    "- **Noise Sensitivity** ‚Äî is the answer affected by irrelevant retrieved content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a22da3ec9394a7f8cbc62246a1fd552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[30]: AttributeError('StringIO' object has no attribute 'classifications')\n",
      "Exception raised in Job[37]: AttributeError('StringIO' object has no attribute 'statements')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.1433, 'faithfulness': 0.3754, 'factual_correctness': 0.5409, 'answer_relevancy': 0.6851, 'context_entity_recall': 0.1887, 'noise_sensitivity_relevant': 0.0000}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    Faithfulness,\n",
    "    FactualCorrectness,\n",
    "    ResponseRelevancy,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity,\n",
    ")\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "\n",
    "baseline_result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[\n",
    "        LLMContextRecall(),\n",
    "        Faithfulness(),\n",
    "        FactualCorrectness(),\n",
    "        ResponseRelevancy(),\n",
    "        ContextEntityRecall(),\n",
    "        NoiseSensitivity(),\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config,\n",
    ")\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Making Adjustments and Re-Evaluating\n",
    "\n",
    "Now that we have a baseline, let's improve the pipeline and measure the impact. We'll make three changes:\n",
    "\n",
    "1. **Larger chunks** (500 characters with 30 overlap instead of 50 with 0 overlap)\n",
    "2. **More documents retrieved** (k=20 instead of k=3)\n",
    "3. **Reranking with Cohere** ‚Äî retrieves 20 documents, then uses Cohere's reranker to select the top 5\n",
    "\n",
    "Reranking is a technique that uses a cross-encoder model (slower but more accurate than embedding similarity) on a smaller subset of candidates to improve retrieval precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved chunking: 202 chunks (vs baseline)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"Improved chunking: {len(split_documents)} chunks (vs baseline)\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "client.create_collection(\n",
    "    collection_name=\"improved_rag\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"improved_rag\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "_ = vector_store.add_documents(documents=split_documents)\n",
    "adjusted_retriever = vector_store.as_retriever(search_kwargs={\"k\": 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers.contextual_compression import (\n",
    "    ContextualCompressionRetriever,\n",
    ")\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "def retrieve_adjusted(state):\n",
    "    compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=adjusted_retriever,\n",
    "        search_kwargs={\"k\": 5},\n",
    "    )\n",
    "    retrieved_docs = compression_retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class AdjustedState(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "adjusted_graph_builder = StateGraph(AdjustedState).add_sequence([retrieve_adjusted, generate])\n",
    "adjusted_graph_builder.add_edge(START, \"retrieve_adjusted\")\n",
    "adjusted_graph = adjusted_graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the improved pipeline works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context does not include specific details on how Stone Ridge approaches risk management in their energy investments.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = adjusted_graph.invoke({\"question\": \"How does Stone Ridge approach risk management in their energy investments?\"})\n",
    "response[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Improved Evaluation\n",
    "\n",
    "Now let's run the same synthetic test set through our improved pipeline and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "rerank_testset = copy.deepcopy(testset)\n",
    "\n",
    "for test_row in rerank_testset:\n",
    "    response = adjusted_graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
    "    test_row.eval_sample.response = response[\"response\"]\n",
    "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
    "    time.sleep(2)  # To avoid rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b03e9df20c741c286df777256a5b2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.6833, 'faithfulness': 0.5825, 'factual_correctness': 0.5364, 'answer_relevancy': 0.7742, 'context_entity_recall': 0.3159, 'noise_sensitivity_relevant': 0.1507}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_evaluation_dataset = EvaluationDataset.from_pandas(rerank_testset.to_pandas())\n",
    "\n",
    "rerank_result = evaluate(\n",
    "    dataset=rerank_evaluation_dataset,\n",
    "    metrics=[\n",
    "        LLMContextRecall(),\n",
    "        Faithfulness(),\n",
    "        FactualCorrectness(),\n",
    "        ResponseRelevancy(),\n",
    "        ContextEntityRecall(),\n",
    "        NoiseSensitivity(),\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config,\n",
    ")\n",
    "rerank_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Question #4:\n",
    "\n",
    "Which system performed better, on what metrics, and why?\n",
    "\n",
    "##### ‚úÖ Answer:\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Question #5:\n",
    "\n",
    "What are the benefits and limitations of using synthetic data generation for RAG evaluation? Consider both the practical advantages and potential pitfalls.\n",
    "\n",
    "##### ‚úÖ Answer:\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Question #6:\n",
    "\n",
    "If you were building a production investment advisory assistant for Stone Ridge, which Ragas metrics would be most important to optimize for and why? Consider the financial services domain specifically.\n",
    "\n",
    "##### ‚úÖ Answer:\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Activity #2: Implement a Different Reranking Strategy\n",
    "\n",
    "Experiment with different reranking parameters or strategies to see how they affect the evaluation metrics.\n",
    "\n",
    "**Requirements:**\n",
    "1. Modify the `retrieve_adjusted` function to use different parameters (e.g., change `k` values, try different `top_n` for reranking)\n",
    "2. Or implement a different retrieval enhancement strategy (e.g., hybrid search, query expansion)\n",
    "3. Run the evaluation and compare results with the baseline and reranking results above\n",
    "4. Document your findings in the markdown cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Implement your custom retrieval strategy here\n",
    "# Example: modify retrieve_adjusted with different parameters\n",
    "\n",
    "def retrieve_custom(state):\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity #2 Findings:\n",
    "\n",
    "*Document your findings here: What strategy did you try? How did it compare to the baseline and reranking results?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, we went end-to-end from data generation to evaluation:\n",
    "\n",
    "1. **Built a knowledge graph** from our investment documents (Stone Ridge 2025 Investor Letter and Alternative Investments Handbook) and used it to understand the structure of our data\n",
    "2. **Generated synthetic test data** with diverse query types (single-hop, multi-hop abstract, multi-hop specific)\n",
    "3. **Built a baseline RAG pipeline** with deliberately simple parameters\n",
    "4. **Evaluated with Ragas** across six metrics to establish a baseline\n",
    "5. **Improved the pipeline** with larger chunks and Cohere reranking\n",
    "6. **Re-evaluated** to measure the impact of our changes\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Synthetic data generation** is critical for early iteration ‚Äî it provides high-quality signal without manually creating test data\n",
    "- **Ragas metrics** give you a multi-dimensional view of RAG quality (retrieval vs. generation vs. faithfulness)\n",
    "- **Small changes matter** ‚Äî chunk size, retrieval strategy, and reranking can dramatically affect evaluation scores\n",
    "- **Always use a different model for judging** than for generating to avoid self-evaluation bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
